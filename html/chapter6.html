<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第6章：传统感知到深度学习感知</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">自动驾驶算法演进史 (2016-2024)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：前深度学习时代与早期探索 (Pre-2016)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：深度学习革命开端 (2016-2018)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：感知架构大爆发 (2019-2020)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：BEV与Transformer变革 (2021-2022)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：端到端浪潮 (2023-2024)</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：传统感知到深度学习感知</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：BEV感知革命与占据网络</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：规划算法 - 从规则到学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：控制算法与执行器协同</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：端到端架构设计与演进</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：端到端工程实践与挑战</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：仿真技术 - 从规则驱动到神经仿真</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：高精地图 vs 无图方案</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：纯视觉感知 - Tesla引领的第一性原理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：激光雷达方案 - 精度与成本的平衡</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：多传感器融合 - 冗余设计的必要性</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第17章：4D毫米波雷达 - 新一代感知利器</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第18章：L2渐进 vs L4跨越 - 自动驾驶的两条道路</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第19章：自动驾驶事故分析与安全挑战</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第20章：特斯拉FSD技术解密</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第21章：Waymo - L4自动驾驶的技术标杆</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第22章：百度Apollo - 从开放平台到商业化落地</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第23章：小鹏汽车 - 从NGP到XNGP的全栈自研之路</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第24章：华为车BU - ADS算法架构深度剖析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第25章：地平线 - 芯片算法协同设计的典范</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第26章：大疆车载 - 极致成本控制下的算法创新</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter27.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第27章：Momenta - 量产与L4双线并进</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter28.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第28章：新势力与传统车企 - 中国自动驾驶的多元化探索</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter29.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第29章：L4公司转型之路</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter30.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第30章：ADAS专业供应商 - 本土化破局与差异化生存</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter31.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第31章：算法与芯片协同演进</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter32.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第32章：大模型与世界模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter33.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第33章：自动驾驶的终局思考</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="6">第6章：传统感知到深度学习感知</h1>
<h2 id="_1">章节概要</h2>
<p>自动驾驶感知技术在2016-2020年间经历了从传统计算机视觉到深度学习的范式转变。本章深入剖析这一转变过程中的关键技术演进、架构创新和工程实践，重点关注2D到3D感知的跃迁、多任务学习架构的兴起，以及深度估计技术如何弥合纯视觉与激光雷达的性能鸿沟。</p>
<h2 id="61-2d3d">6.1 从2D检测到3D感知</h2>
<h3 id="611-cv-pre-2016">6.1.1 传统CV时代的局限性 (Pre-2016)</h3>
<h4 id="_2">经典方法回顾</h4>
<p>传统计算机视觉方法在自动驾驶早期扮演了重要角色，但存在根本性局限：</p>
<pre class="codehilite"><code>传统CV Pipeline (2010-2015)
┌─────────────┐    ┌──────────────┐    ┌───────────┐
│  特征提取    │ -&gt; │  特征描述     │ -&gt; │  分类器    │
│  HOG/SIFT   │    │  BoW/Fisher  │    │  SVM/RF   │
└─────────────┘    └──────────────┘    └───────────┘
      ↓                    ↓                  ↓
  手工设计            维度诅咒           泛化能力差
</code></pre>

<p><strong>HOG (Histogram of Oriented Gradients) 行人检测</strong></p>
<ul>
<li>Dalal &amp; Triggs (2005) 的经典方法</li>
<li>滑动窗口 + 梯度直方图</li>
<li>检测速度: ~1 FPS (640×480)</li>
<li>检测精度: ~80% AP on INRIA dataset</li>
<li>致命缺陷: 无法处理遮挡、形变、光照变化</li>
</ul>
<p>实际工程实现中的细节:</p>
<pre class="codehilite"><code>HOG特征提取流程:

1. 梯度计算: Sobel算子获取Gx, Gy
2. Cell划分: 8×8像素为一个cell
3. 直方图: 9个bin统计梯度方向
4. Block归一化: 2×2 cells组成block, L2-norm
5. 特征向量: 15×7 blocks × 36维 = 3780维
</code></pre>

<p><strong>Haar Cascades 车辆检测</strong></p>
<ul>
<li>Viola-Jones框架的延伸</li>
<li>积分图加速计算</li>
<li>实时性好但精度低</li>
<li>MobileEye早期EyeQ芯片的主要算法</li>
</ul>
<p>Haar特征的局限性:</p>
<ul>
<li>仅使用矩形特征，表达能力有限</li>
<li>对旋转和尺度变化敏感</li>
<li>级联分类器训练耗时(数周)</li>
<li>误检率高，需要大量后处理</li>
</ul>
<p><strong>立体视觉深度估计</strong></p>
<ul>
<li>基于SGBM (Semi-Global Block Matching)</li>
<li>计算复杂度O(WHD), D为视差搜索范围</li>
<li>对标定精度极度敏感</li>
<li>纹理缺失区域失效</li>
</ul>
<p>SGBM算法核心:</p>
<pre class="codehilite"><code>能量函数: E(D) = ΣE_data(p,Dp) + ΣE_smooth(Dp,Dq)
其中:

- E_data: 匹配代价(Census/SAD)
- E_smooth: 平滑约束
- 8方向动态规划聚合
</code></pre>

<p><strong>光流法运动估计</strong></p>
<p>早期ADAS系统大量使用光流进行:</p>
<ul>
<li>自身运动估计(Visual Odometry)</li>
<li>移动物体检测</li>
<li>碰撞时间(TTC)预测</li>
</ul>
<p>Lucas-Kanade光流:</p>
<pre class="codehilite"><code>假设: 局部区域内光流恒定
求解: [Ix Iy][u v]' = -It
问题: 孔径问题、光照变化、大位移
</code></pre>

<p>实际应用案例:</p>
<ul>
<li>2014 Mercedes S-Class: 立体视觉+光流</li>
<li>2015 Audi Q7: 单目视觉+SfM</li>
<li>早期Tesla AP1.0: MobileEye EyeQ3混合方案</li>
</ul>
<h4 id="mobileeye-eyeq-2014-2017">MobileEye EyeQ时代 (2014-2017)</h4>
<p>MobileEye在深度学习来临前的统治地位源于其精心设计的专用架构：</p>
<p>| EyeQ版本 | 年份 | 算力 | 核心算法 | 客户 |</p>
<table>
<thead>
<tr>
<th>EyeQ版本</th>
<th>年份</th>
<th>算力</th>
<th>核心算法</th>
<th>客户</th>
</tr>
</thead>
<tbody>
<tr>
<td>EyeQ2</td>
<td>2010</td>
<td>2.5 GOPS</td>
<td>Haar+HOG</td>
<td>BMW, GM</td>
</tr>
<tr>
<td>EyeQ3</td>
<td>2014</td>
<td>256 GOPS</td>
<td>混合CNN</td>
<td>Tesla AP1</td>
</tr>
<tr>
<td>EyeQ4</td>
<td>2018</td>
<td>2.5 TOPS</td>
<td>深度CNN</td>
<td>日产ProPilot</td>
</tr>
<tr>
<td>EyeQ5</td>
<td>2021</td>
<td>24 TOPS</td>
<td>全CNN+REM</td>
<td>BMW, NIO</td>
</tr>
</tbody>
</table>
<p><strong>EyeQ3架构深度剖析</strong></p>
<p>EyeQ3的突破在于引入了早期CNN，但仍保留大量传统CV：</p>
<pre class="codehilite"><code>EyeQ3 处理流水线:
┌──────────────────────────────────────┐
│  Image Preprocessing                  │
│  - 去马赛克(Bayer-&gt;RGB)               │
│  - 降噪、HDR合成                      │
└────────────┬─────────────────────────┘
             ↓
     ┌───────┴────────┐
     ↓                ↓
┌────────────┐  ┌──────────────┐
│ 传统CV分支  │  │  CNN分支      │
│ - 车道线    │  │ - 车辆检测    │
│ - 路沿      │  │ - 行人检测    │  
│ - 标志牌    │  │ - 交通灯      │
└────────────┘  └──────────────┘
     ↓                ↓
     └───────┬────────┘
             ↓
      融合与跟踪模块
</code></pre>

<p>关键技术细节:</p>
<ul>
<li>车道线: 基于Hough变换的多项式拟合</li>
<li>RANSAC鲁棒估计</li>
<li>3次多项式: y = ax³ + bx² + cx + d</li>
<li>时序平滑: 卡尔曼滤波</li>
<li>目标跟踪: Kalman滤波 + Hungarian匹配</li>
<li>状态向量: [x, y, vx, vy, w, h]</li>
<li>数据关联: IoU + 外观特征</li>
<li>空闲空间: 基于v-disparity的地面估计</li>
<li>Stixel World表示</li>
<li>动态规划优化</li>
</ul>
<p><strong>REM (Road Experience Management)</strong></p>
<p>MobileEye独特的众包地图策略:</p>
<pre class="codehilite"><code>车端采集 -&gt; 特征提取 -&gt; 上传云端 -&gt; 地图构建
   ↓           ↓           ↓          ↓
摄像头图像  稀疏特征    10KB/km    HD Map
</code></pre>

<p>特点:</p>
<ul>
<li>轻量级: 仅传输语义特征，不传图像</li>
<li>实时更新: 众包模式，百万级车辆</li>
<li>低成本: 无需专业采集车</li>
</ul>
<h3 id="612-2d-2016-2018">6.1.2 深度学习2D检测革命 (2016-2018)</h3>
<p>这一时期见证了深度学习彻底改变目标检测的过程，从学术突破到工程落地仅用了2年时间。</p>
<h4 id="yolo">YOLO系列：实时检测的突破</h4>
<pre class="codehilite"><code>YOLO演进时间线
2016.6  YOLOv1  45 FPS  63.4% mAP  端到端训练
   ↓
2016.12 YOLOv2  67 FPS  76.8% mAP  Anchor boxes
   ↓  
2018.4  YOLOv3  65 FPS  82.5% mAP  多尺度预测
   ↓
2020.4  YOLOv4  65 FPS  87.2% mAP  CSPNet backbone
</code></pre>

<p><strong>YOLOv1的革命性创新</strong></p>
<p>将检测转化为回归问题:</p>
<pre class="codehilite"><code>输入: 448×448图像
   ↓
划分: 7×7网格
   ↓  
输出: 每个网格预测:

      - 2个bbox: (x,y,w,h,confidence)
      - 20类概率 (PASCAL VOC)
   ↓
NMS后处理
</code></pre>

<p>局限性:</p>
<ul>
<li>每个网格只能检测2个物体</li>
<li>小物体检测效果差</li>
<li>定位精度不如two-stage</li>
</ul>
<p><strong>YOLOv2/v3关键改进</strong></p>
<p>YOLOv2 (YOLO9000):</p>
<ul>
<li>Anchor boxes: 使用k-means聚类得到5个anchors</li>
<li>Batch Normalization: 每层后加BN, mAP提升2%</li>
<li>Multi-scale training: 随机选择输入尺度</li>
<li>Darknet-19: 更深的backbone</li>
</ul>
<p>YOLOv3重大升级:</p>
<pre class="codehilite"><code>多尺度检测结构:
    Darknet-53
         ↓
    ┌────┬─────┐  
    ↓    ↓     ↓
  52×52 26×26 13×13  &lt;- 三个尺度
    ↓    ↓     ↓
  小物体 中物体 大物体
</code></pre>

<p><strong>YOLOv3在自动驾驶中的应用</strong></p>
<p><em>Tesla Autopilot 2.0 (2017)</em>:</p>
<ul>
<li>8摄像头YOLOv3变体</li>
<li>每个摄像头独立处理</li>
<li>主前视高分辨率(1280×960)</li>
<li>周视低分辨率(640×480)</li>
<li>HW2.5: NVIDIA Drive PX2</li>
</ul>
<p><em>百度Apollo 2.0 (2018)</em>:</p>
<ul>
<li>YOLOv3 + Deep SORT跟踪</li>
<li>融合激光雷达点云</li>
<li>关键改进: </li>
<li>Darknet-53 backbone适配车载算力</li>
<li>FPN多尺度检测小目标</li>
<li>9个anchor覆盖车辆尺度范围</li>
<li>自定义类别: 车、人、自行车、交通锥</li>
</ul>
<p>车载部署优化:</p>
<pre class="codehilite"><code class="language-python"># TensorRT优化
config.set_flag(trt.BuilderFlag.FP16)
config.set_flag(trt.BuilderFlag.STRICT_TYPES)
# 量化感知训练(QAT)
# INT8推理，速度提升3×
</code></pre>

<h4 id="two-stage">Two-Stage方法的精度优势</h4>
<p>Faster R-CNN系列在需要高精度的场景仍有优势：</p>
<pre class="codehilite"><code>Two-Stage Pipeline
┌────────┐    ┌─────────┐    ┌──────────┐
│ RPN    │ -&gt; │ RoI Pool│ -&gt; │ R-CNN    │
│ 候选区  │    │ 特征提取 │    │ 精细分类  │
└────────┘    └─────────┘    └──────────┘
    ↓              ↓              ↓
1000个候选     7×7特征图     类别+bbox
</code></pre>

<p><strong>Faster R-CNN核心创新</strong></p>
<p>RPN (Region Proposal Network):</p>
<pre class="codehilite"><code>共享卷积特征
      ↓
  3×3滑动窗口
      ↓
  9个anchors
  (3 scales × 3 ratios)
      ↓
  2分类 + 4回归
</code></pre>

<p>RoI Pooling问题:</p>
<ul>
<li>量化误差大</li>
<li>小物体特征丢失</li>
</ul>
<p>RoI Align解决方案:</p>
<ul>
<li>双线性插值</li>
<li>保持空间对齐</li>
<li>mAP提升~1%</li>
</ul>
<p><strong>FPN (Feature Pyramid Networks)</strong></p>
<pre class="codehilite"><code>自底向上路径          自顶向下路径
                        P5 ←──┐
C5 (stride=32) ─────→       ↓
                        P4 ←──┼──┐ 
C4 (stride=16) ─────→       ↓   ↓
                        P3 ←──┼──┼─┐
C3 (stride=8)  ─────→       ↓   ↓ ↓
                        P2 ←──┘   ↓ ↓
C2 (stride=4)  ─────→           ↓ ↓
                              检测头
</code></pre>

<p>优势:</p>
<ul>
<li>多尺度特征融合</li>
<li>小物体检测大幅提升</li>
<li>计算成本增加少</li>
</ul>
<p><strong>Cascade R-CNN在L4系统的应用</strong></p>
<p>级联结构:</p>
<pre class="codehilite"><code>RPN -&gt; H1(IoU=0.5) -&gt; H2(IoU=0.6) -&gt; H3(IoU=0.7)
         ↓              ↓              ↓
      粗检测          精细化         更精细
</code></pre>

<p><em>Waymo (2018-2019)</em>:</p>
<ul>
<li>Cascade R-CNN处理稀疏激光雷达</li>
<li>点云体素化后输入CNN</li>
<li>IoU阈值递增: 0.5 -&gt; 0.6 -&gt; 0.7</li>
<li>小目标检测提升: +3.2% AP on KITTI</li>
<li>边界框精度提升: +5% localization</li>
</ul>
<p><em>Cruise (2019)</em>:</p>
<ul>
<li>多传感器Cascade R-CNN</li>
<li>Camera + LiDAR early fusion</li>
<li>每个stage不同模态特征</li>
</ul>
<h3 id="613-2d3d-2018-2020">6.1.3 从2D到3D的关键跨越 (2018-2020)</h3>
<p>3D感知是自动驾驶的核心挑战，从2D到3D的跨越涉及深度估计、坐标转换、遮挡处理等多个难题。</p>
<h4 id="3d">单目3D检测的探索</h4>
<p>单目3D检测是计算机视觉的"圣杯"问题之一，需要从单张图像恢复完整的3D信息。</p>
<p><strong>核心挑战</strong></p>
<ol>
<li>尺度模糊性: 远处的大车vs近处的小车</li>
<li>深度估计: 缺少立体基线</li>
<li>遮挡问题: 部分可见物体的3D框</li>
<li>姿态估计: 物体朝向的准确预测</li>
</ol>
<p><strong>M3D-RPN (2019)</strong></p>
<p>创新的2D-3D锚点设计:</p>
<pre class="codehilite"><code>深度感知锚点生成
┌─────────────────────────────┐
│  2D中心 (u,v)                │
│     ↓ 相机内参K              │
│  3D射线方向                   │
│     ↓ 统计先验               │
│  3D中心 (X,Y,Z)              │
│     ↓ 尺寸先验               │
│  3D框 (l,w,h,θ)              │
└─────────────────────────────┘

关键约束:

1. 2D-3D一致性: 3D框投影=2D检测框
2. 深度有序性: 前后关系约束
3. 地面约束: 车辆贴地假设
</code></pre>

<p>深度推理策略:</p>
<ul>
<li>利用物体类别的平均尺寸</li>
<li>根据2D框大小推断深度</li>
<li>公式: depth = (f × height_3d) / height_2d</li>
</ul>
<p>性能:</p>
<ul>
<li>KITTI Easy: 20.27% AP3D</li>
<li>KITTI Moderate: 17.06% AP3D  </li>
<li>KITTI Hard: 15.21% AP3D</li>
</ul>
<p><strong>SMOKE (2020) - 无需2D检测的3D预测</strong></p>
<p>革命性的单阶段设计:</p>
<pre class="codehilite"><code>架构流程:
Image → DLA-34 Backbone → Keypoint Heatmap
                        ↓
                   3D Centers (投影点)
                        ↓
                   3D Attributes

                   - depth
                   - dimensions (l,w,h)
                   - orientation
</code></pre>

<p>关键创新:</p>
<ul>
<li>直接回归3D中心投影点(不是2D框中心!)</li>
<li>关键见解: 3D中心投影≠2D bbox中心</li>
<li>消除2D检测的误差累积</li>
<li>端到端可微分训练</li>
</ul>
<p>损失函数设计:</p>
<pre class="codehilite"><code class="language-python">L_total = L_cls + λ₁L_reg + λ₂L_off
其中:

- L_cls: focal loss for keypoints
- L_reg: L1 loss for 3D attributes
- L_off: sub-pixel offset
</code></pre>

<p>性能指标:</p>
<ul>
<li>KITTI moderate: 14.03% AP3D</li>
<li>推理速度: 30ms on 1080Ti</li>
<li>内存占用: 仅需2GB GPU内存</li>
</ul>
<p><strong>MonoDIS (2019)</strong></p>
<p>解耦的3D检测:</p>
<pre class="codehilite"><code>2D Detection → 2D-3D Lifting → 3D Refinement
     ↓              ↓                ↓
  2D boxes    Initial 3D      Refined 3D
              w/ uncertainty
</code></pre>

<p>不确定性建模:</p>
<ul>
<li>Aleatoric uncertainty: 数据噪声</li>
<li>Epistemic uncertainty: 模型不确定性</li>
<li>用于后处理的置信度加权</li>
</ul>
<h4 id="_3">伪激光雷达方法</h4>
<p><strong>Pseudo-LiDAR (2019)</strong></p>
<p>将深度图转换为点云表示的革命性思路：</p>
<pre class="codehilite"><code>Pipeline:
图像对 -&gt; 深度估计 -&gt; 3D点云 -&gt; PointNet检测
        PSMNet     坐标变换    现成3D检测器

关键创新: 表示形式比深度精度更重要
性能提升: 单目3D检测 +100% AP3D
</code></pre>

<p><strong>核心洞察</strong></p>
<p>为什么表示形式如此重要？</p>
<pre class="codehilite"><code>前视图(图像空间)的问题:
┌────────────────┐
│ ▪▪▪▪▪▪▪▪▪▪▪▪  │ &lt;- 远处物体压缩
│  ▪▪▪▪▪▪▪▪▪    │
│   ▪▪▪▪▪▪▪     │ &lt;- 透视畸变
│    ██████      │ &lt;- 近处物体
└────────────────┘

鸟瞰图(BEV空间)的优势:
┌────────────────┐
│ · · · · · · ·  │ &lt;- 均匀分布
│ · · ■ · · · ·  │ &lt;- 无畸变
│ · · · · ■ · ·  │ &lt;- 保持相对位置
│ · · · · · · ·  │
└────────────────┘
</code></pre>

<p><strong>技术实现细节</strong></p>
<p>坐标变换公式:</p>
<pre class="codehilite"><code class="language-python">def image_to_lidar(depth, intrinsics, extrinsics):
    # 生成像素网格
    h, w = depth.shape
    i, j = np.meshgrid(np.arange(w), np.arange(h))

    # 反投影到相机坐标系
    z = depth
    x = (i - intrinsics[0,2]) * z / intrinsics[0,0]
    y = (j - intrinsics[1,2]) * z / intrinsics[1,1]

    # 相机坐标到激光雷达坐标
    pts_cam = np.stack([x, y, z, np.ones_like(z)], axis=-1)
    pts_lidar = pts_cam @ extrinsics.T

    return pts_lidar[:,:,:3]
</code></pre>

<p><strong>Pseudo-LiDAR++改进</strong></p>
<p>多模态深度增强:</p>
<pre class="codehilite"><code>立体图像 ────┐
            ├→ Depth CNN → 初始深度
稀疏LiDAR ────┘              ↓
                     Depth Completion
                            ↓
                      精细化深度图
                            ↓
                     伪激光雷达点云
</code></pre>

<p>关键技术:</p>
<ol>
<li>
<p><strong>深度补全网络</strong>
   - 4线激光雷达提供稀疏但准确的深度
   - CNN学习从稀疏到稠密的映射
   - 性能提升: +15% 3D AP</p>
</li>
<li>
<p><strong>Foreground Point Segmentation</strong>
   - 使用2D检测mask过滤背景点
   - 减少前景/背景混淆
   - 边界准确度提升30%</p>
</li>
<li>
<p><strong>Cost Volume Filtering</strong></p>
</li>
</ol>
<pre class="codehilite"><code>立体匹配代价体 → 3D CNN滤波 → 置信度图
                                 ↓
                           加权点云生成
</code></pre>

<p>性能对比:
| 方法 | 输入 | 3D AP (Moderate) |</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>输入</th>
<th>3D AP (Moderate)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pseudo-LiDAR</td>
<td>单目</td>
<td>28.31%</td>
</tr>
<tr>
<td>Pseudo-LiDAR</td>
<td>立体</td>
<td>42.43%</td>
</tr>
<tr>
<td>Pseudo-LiDAR++</td>
<td>立体+4线</td>
<td>51.75%</td>
</tr>
<tr>
<td>真实64线LiDAR</td>
<td>LiDAR</td>
<td>82.58%</td>
</tr>
</tbody>
</table>
<h3 id="614-3d-2019-2020">6.1.4 多视角3D感知 (2019-2020)</h3>
<p>多视角3D感知标志着从单相机到环视系统的重要转变，为后续的BEV感知奠定基础。</p>
<h4 id="lss-bev">LSS: BEV感知的先驱</h4>
<p><strong>Lift-Splat-Shoot (2020)</strong></p>
<p>LSS开创性地提出了可微分的视角转换方法：</p>
<pre class="codehilite"><code>详细的三步转换过程:

1. Lift: 图像特征 -&gt; 3D frustum特征
   ┌─────────────────────────────────┐
   │ 对每个像素(u,v):                 │
   │ - 提取特征向量 f(u,v)            │
   │ - 预测深度分布 D=[d₁...dₙ]      │
   │ - 预测深度概率 α=[α₁...αₙ]      │
   │ - 生成3D点: Σαᵢ·f⊗(u,v,dᵢ)     │
   └─────────────────────────────────┘

2. Splat: 3D特征 -&gt; BEV网格
   ┌─────────────────────────────────┐
   │ Voxel Pooling:                  │
   │ - 将3D点分配到BEV网格           │
   │ - 每个网格累加特征               │
   │ - 200×200×1 BEV特征图           │
   └─────────────────────────────────┘

3. Shoot: BEV特征 -&gt; 下游任务
   ┌─────────────────────────────────┐
   │ 任务头:                         │
   │ - 目标检测: CenterPoint style   │
   │ - 语义分割: U-Net decoder       │
   │ - 运动规划: Cost volume         │
   └─────────────────────────────────┘
</code></pre>

<p><strong>关键技术创新</strong></p>
<ol>
<li><strong>离散深度分布</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># 不是预测单一深度值
depth_logits = model(image)  # [B,D,H,W]
depth_prob = softmax(depth_logits, dim=1)

# D个离散深度假设
depth_bins = [4.0, 8.0, 12.0, ..., 45.0]  # 米
</code></pre>

<ol start="2">
<li><strong>外积构造3D特征</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># 图像特征: [C, H, W]
# 深度概率: [D, H, W]
# 3D特征: [C, D, H, W]
feat_3d = feat_2d.unsqueeze(1) * depth_prob.unsqueeze(0)
</code></pre>

<ol start="3">
<li><strong>Voxel Pooling</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># 累加到BEV网格
for each 3d_point:
    bev_x, bev_y = world_to_bev(point.xyz)
    bev_features[bev_x, bev_y] += point.features
</code></pre>

<p>性能指标:</p>
<ul>
<li>nuScenes Detection: 32.1 NDS</li>
<li>BEV Segmentation: 29.5 mIoU</li>
<li>6相机推理: 35 FPS on 2080Ti</li>
</ul>
<p>关键突破:</p>
<ul>
<li>深度分布而非单一深度值(鲁棒性↑)</li>
<li>可微分的视角转换(端到端训练)</li>
<li>统一的多任务表示(效率↑)</li>
</ul>
<h4 id="detr3d-query-based-3d">DETR3D: Query-based 3D检测</h4>
<p>DETR3D将Transformer引入3D检测，开创了query-based的新范式：</p>
<pre class="codehilite"><code>完整架构流程:

Multi-view Images (6 cameras)
        ↓
  ResNet-101 + FPN
        ↓
  Image Features [6, C, H, W]
        ↓
  ┌──────────────────────┐
  │  3D-2D Transform     │
  │  - 3D ref points     │
  │  - Camera projection │
  │  - Feature sampling  │
  └──────────────────────┘
        ↓
Transformer Decoder (6 layers)
        ↓
  Object Queries (900)
        ↓
  3D Boxes + Classes
</code></pre>

<p><strong>核心创新</strong></p>
<ol>
<li><strong>3D Reference Points</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># 可学习的3D参考点
ref_points = nn.Parameter(torch.randn(900, 3))

# 投影到各相机
for cam in cameras:
    uv = project_3d_to_2d(ref_points, cam.intrinsic, cam.extrinsic)
    features = bilinear_sample(cam.features, uv)
</code></pre>

<ol start="2">
<li><strong>Multi-Scale 3D位置编码</strong></li>
</ol>
<pre class="codehilite"><code>3D PE = sin(2πk·[x,y,z]/λ)
其中λ从小到大，捕获不同尺度
</code></pre>

<ol start="3">
<li><strong>Set-to-Set Loss</strong>
   - Hungarian匹配
   - 分类loss + L1 box loss + GIoU loss</li>
</ol>
<p>优势:</p>
<ul>
<li>无需密集深度估计</li>
<li>全局感受野和关系建模</li>
<li>端到端优化</li>
<li>自然处理遮挡</li>
</ul>
<p>性能:</p>
<ul>
<li>nuScenes test: 41.2 NDS, 34.9 mAP</li>
<li>推理速度: 27 FPS</li>
</ul>
<p><strong>BEVFormer延伸</strong></p>
<p>BEVFormer在DETR3D基础上引入时序和BEV queries：</p>
<pre class="codehilite"><code>Temporal Self-Attention
    ↓
Spatial Cross-Attention  
    ↓
BEV Queries [200×200]
    ↓
统一BEV特征
</code></pre>

<p>创新:</p>
<ul>
<li>BEV queries作为统一表示</li>
<li>时序信息融合</li>
<li>可变形注意力加速</li>
</ul>
<h2 id="62">6.2 多任务学习与特征共享</h2>
<h3 id="621">6.2.1 多任务学习动机</h3>
<h4 id="_4">计算资源约束</h4>
<p>车载平台算力限制下的权衡：</p>
<p>| 平台 | 算力 | 功耗 | 单任务模型数 | 多任务收益 |</p>
<table>
<thead>
<tr>
<th>平台</th>
<th>算力</th>
<th>功耗</th>
<th>单任务模型数</th>
<th>多任务收益</th>
</tr>
</thead>
<tbody>
<tr>
<td>Xavier</td>
<td>30 TOPS</td>
<td>30W</td>
<td>3-4个</td>
<td>基准</td>
</tr>
<tr>
<td>Orin</td>
<td>254 TOPS</td>
<td>60W</td>
<td>8-10个</td>
<td>40%算力节省</td>
</tr>
<tr>
<td>J5</td>
<td>128 TOPS</td>
<td>35W</td>
<td>5-6个</td>
<td>50%算力节省</td>
</tr>
</tbody>
</table>
<h4 id="_5">任务相关性分析</h4>
<pre class="codehilite"><code>感知任务相关性矩阵
        检测  分割  深度  车道线
检测    1.0   0.7   0.8   0.5
分割    0.7   1.0   0.6   0.7  
深度    0.8   0.6   1.0   0.4
车道线  0.5   0.7   0.4   1.0

高相关性(&gt;0.7)任务适合共享
</code></pre>

<h3 id="622-2018-2019">6.2.2 早期多任务架构 (2018-2019)</h3>
<h4 id="multinet-2018">MultiNet (2018)</h4>
<pre class="codehilite"><code>共享编码器架构
           Input
             ↓
     Shared Encoder
          /  |  \
    检测头  分割头  深度头
       ↓     ↓      ↓
    Boxes  Mask   Depth
</code></pre>

<p>特点：</p>
<ul>
<li>简单的hard parameter sharing</li>
<li>任务间无交互</li>
<li>梯度冲突问题严重</li>
</ul>
<h4 id="dlt-net-2019">DLT-Net (2019)</h4>
<p>引入可学习的任务权重：</p>
<pre class="codehilite"><code class="language-python"># 动态任务权重
task_weights = {
    'detection': 1.0,
    'drivable': 0.7 * (1 + 0.3*cos(epoch/max_epoch*π)),
    'lane': 0.5
}
</code></pre>

<h3 id="623-2019-2020">6.2.3 注意力机制与任务交互 (2019-2020)</h3>
<h4 id="pad-net">PAD-Net架构</h4>
<pre class="codehilite"><code>任务交互模块
Detection Features ←→ Distillation ←→ Segmentation Features
         ↓                ↓                    ↓
    Det Output      Shared Feature       Seg Output
</code></pre>

<p>关键创新：</p>
<ul>
<li>Task-specific attention masks</li>
<li>Cross-task feature distillation  </li>
<li>性能提升: +2.3% mAP, +1.8% mIoU</li>
</ul>
<h4 id="yolop-2021">YOLOP (2021)</h4>
<pre class="codehilite"><code>高效的三任务模型
┌──────────────────────────────┐
│     Shared Backbone          │
│      CSPDarknet              │
└──────────┬───────────────────┘
           ↓
    ┌──────┴──────┐
    ↓             ↓
 Neck(FPN)    Seg Decoder
    ↓             ↓
 Det Head    DA&amp;LL Head
    ↓          ↓    ↓
 Boxes    Drivable Lane

推理速度: 40ms@320×180 (Jetson TX2)
精度: 89.2 mAP, 91.5 mIoU drivable, 70.3 IoU lane
</code></pre>

<h3 id="624">6.2.4 端到端多任务优化</h3>
<h4 id="_6">梯度平衡策略</h4>
<p><strong>GradNorm (2018)</strong></p>
<pre class="codehilite"><code class="language-python"># 自适应任务权重
L_total = Σ w_i(t) * L_i
w_i(t+1) = w_i(t) * exp(α * (G_i/G_avg - 1))
# G_i: 任务i的梯度范数
</code></pre>

<p><strong>Uncertainty Weighting</strong></p>
<pre class="codehilite"><code class="language-python"># 基于不确定性的权重
L_total = Σ (1/2σ_i²) * L_i + log(σ_i)
# σ_i: 可学习的任务不确定性
</code></pre>

<h4 id="_7">多任务学习收益分析</h4>
<p>| 方法 | 检测mAP | 分割mIoU | 深度RMSE | FPS | 内存 |</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>检测mAP</th>
<th>分割mIoU</th>
<th>深度RMSE</th>
<th>FPS</th>
<th>内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>3个独立模型</td>
<td>78.2</td>
<td>89.3</td>
<td>4.82</td>
<td>12</td>
<td>3.2GB</td>
</tr>
<tr>
<td>Hard Sharing</td>
<td>76.5</td>
<td>87.1</td>
<td>5.13</td>
<td>35</td>
<td>1.1GB</td>
</tr>
<tr>
<td>YOLOP</td>
<td>77.8</td>
<td>88.9</td>
<td>-</td>
<td>40</td>
<td>0.9GB</td>
</tr>
<tr>
<td>HybridNets</td>
<td>77.3</td>
<td>85.8</td>
<td>5.54</td>
<td>32</td>
<td>1.2GB</td>
</tr>
</tbody>
</table>
<h2 id="63">6.3 深度估计与伪激光雷达</h2>
<h3 id="631">6.3.1 单目深度估计演进</h3>
<h4 id="2016-2018">监督学习方法 (2016-2018)</h4>
<p><strong>DORN (Deep Ordinal Regression Network)</strong></p>
<pre class="codehilite"><code>深度离散化策略
连续深度 -&gt; 序数标签 -&gt; 分类问题
[0,80m] -&gt; 80个bins -&gt; Softmax

关键创新:

- Spacing-Increasing Discretization
- 近处密集，远处稀疏
- KITTI: 4.46m RMSE
</code></pre>

<p><strong>BTS (2019)</strong></p>
<pre class="codehilite"><code>架构创新:
Encoder   Decoder
ResNet → Upsampling
   ↓        ↑
 Skip → Local Planar
       Guidance

LPG层: 平面假设指导上采样
性能: KITTI 2.21m RMSE
</code></pre>

<h4 id="2017-2020">自监督深度估计 (2017-2020)</h4>
<p><strong>Monodepth2 (2019)</strong></p>
<pre class="codehilite"><code>自监督损失设计:
L_total = L_photo + L_smooth + L_consistency

L_photo: 光度一致性
L_smooth: 深度平滑
L_consistency: 左右一致性

训练数据: 仅需单目视频
性能: 接近监督方法80%
</code></pre>

<p><strong>关键技术突破</strong></p>
<ol>
<li><strong>遮挡处理</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># Minimum reprojection loss
L_photo = min(||I_t - I'_t→t-1||, ||I_t - I'_t→t+1||)
</code></pre>

<ol start="2">
<li><strong>移动物体处理</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># Auto-masking
mask = (L_photo &lt; L_identity)
</code></pre>

<ol start="3">
<li><strong>尺度一致性</strong>
- 通过已知相机高度恢复绝对尺度
- 或使用车速信息作为尺度监督</li>
</ol>
<h3 id="632">6.3.2 立体深度估计</h3>
<h4 id="_8">传统立体匹配回顾</h4>
<pre class="codehilite"><code>经典Pipeline:
左图 ──┐
      ├→ 特征提取 → 代价计算 → 代价聚合 → 视差优化
右图 ──┘
        ↓           ↓          ↓          ↓
     Census    SAD/SSD      SGM       左右检查
</code></pre>

<p>问题：</p>
<ul>
<li>纹理缺失区域失效</li>
<li>遮挡边界错误</li>
<li>计算复杂度高</li>
</ul>
<h4 id="_9">深度学习立体匹配</h4>
<p><strong>PSMNet (2018)</strong></p>
<pre class="codehilite"><code>金字塔立体匹配网络
┌─────────────────────────┐
│   Spatial Pyramid       │
│   Pooling Module        │
└───────┬─────────────────┘
        ↓
   Cost Volume (D×H×W)
        ↓
   3D CNN Aggregation
        ↓
   Disparity Regression
</code></pre>

<p>创新点：</p>
<ul>
<li>空间金字塔池化扩大感受野</li>
<li>3D卷积代价聚合</li>
<li>亚像素精度: soft argmin</li>
<li>KITTI 2015: 1.86 pixel error</li>
</ul>
<p><strong>GA-Net (2019)</strong></p>
<pre class="codehilite"><code>引导聚合网络
Semi-Global → Learnable
   ↓            ↓
 GA Layer: 可学习的聚合方向

性能提升但计算量大
推理: 600ms @ 2080Ti
</code></pre>

<h3 id="633">6.3.3 伪激光雷达技术深度剖析</h3>
<h4 id="_10">核心思想与创新</h4>
<p><strong>表示形式的重要性</strong></p>
<pre class="codehilite"><code>深度图表示 vs 点云表示

深度图 (前视图)          点云 (鸟瞰图)
┌────────────┐         ┌────────────┐
│░░░░████░░░░│         │  ∙∙∙∙∙∙∙   │
│░░████████░░│   →     │ ∙∙∙∙∙∙∙∙∙  │
│████████████│         │∙∙∙∙∙∙∙∙∙∙∙ │
└────────────┘         └────────────┘
   透视畸变                均匀分布

关键洞察: 

- 3D检测器对点云表示的归纳偏置
- BEV视角避免透视畸变
</code></pre>

<h4 id="_11">技术实现细节</h4>
<p><strong>坐标转换</strong></p>
<pre class="codehilite"><code class="language-python"># 深度图到点云
def depth_to_pointcloud(depth, K):
    h, w = depth.shape
    u, v = np.meshgrid(range(w), range(h))

    # 反投影到3D
    z = depth
    x = (u - K[0,2]) * z / K[0,0]  
    y = (v - K[1,2]) * z / K[1,1]

    # 相机坐标系到激光雷达坐标系
    points = np.stack([z, -x, -y], axis=-1)
    return points.reshape(-1, 3)
</code></pre>

<p><strong>性能对比</strong></p>
<p>| 方法 | 输入 | 3D AP (Mod) | 推理时间 |</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>输入</th>
<th>3D AP (Mod)</th>
<th>推理时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>M3D-RPN</td>
<td>单目</td>
<td>14.76</td>
<td>160ms</td>
</tr>
<tr>
<td>Pseudo-LiDAR</td>
<td>单目</td>
<td>28.31</td>
<td>400ms</td>
</tr>
<tr>
<td>Pseudo-LiDAR</td>
<td>立体</td>
<td>42.43</td>
<td>450ms</td>
</tr>
<tr>
<td>PointPillars</td>
<td>64线LiDAR</td>
<td>82.58</td>
<td>40ms</td>
</tr>
</tbody>
</table>
<h4 id="_12">改进与优化</h4>
<p><strong>PL++关键改进</strong></p>
<ol>
<li><strong>深度补全网络</strong></li>
</ol>
<pre class="codehilite"><code>稀疏LiDAR + 深度图 → 稠密深度
4线激光雷达指导立体匹配
性能提升: +15% 3D AP
</code></pre>

<ol start="2">
<li><strong>前景分割</strong></li>
</ol>
<pre class="codehilite"><code>Instance mask指导深度估计
避免前景/背景深度混淆
边界准确度提升30%
</code></pre>

<ol start="3">
<li><strong>时序融合</strong></li>
</ol>
<pre class="codehilite"><code class="language-python"># 多帧深度融合
depth_t = α*depth_t + (1-α)*warp(depth_t-1)
# 提升远距离深度稳定性
</code></pre>

<h3 id="634">6.3.4 深度估计在量产中的应用</h3>
<h4 id="tesla-fsd">Tesla FSD的深度估计策略</h4>
<p><strong>HydraNet架构 (2019-2021)</strong></p>
<pre class="codehilite"><code>8摄像头输入
     ↓
Shared Backbone
     ↓
摄像头专属头部 × 8
     ↓
深度+检测+分割

关键技术:

- 相机间深度一致性约束
- 运动立体增强单目深度
- 实时性: 36 FPS @ HW3.0
</code></pre>

<h4 id="_13">中国方案实践</h4>
<p><strong>小鹏XPILOT深度方案</strong></p>
<pre class="codehilite"><code>三支路融合:

1. 单目深度网络 (全场景)
2. 环视立体匹配 (停车场景)
3. 结构约束 (车道线/路沿)
     ↓
  统一深度图
</code></pre>

<p><strong>地平线深度估计加速</strong></p>
<pre class="codehilite"><code>量化策略:
FP32 → INT8

- 深度回归头保持FP16
- Backbone INT8量化
- 性能损失 &lt;2%
- 速度提升 3.5×
</code></pre>

<h2 id="64">6.4 工程化实践与挑战</h2>
<h3 id="641">6.4.1 数据工程</h3>
<h4 id="_14">深度真值获取</h4>
<p><strong>激光雷达投影</strong></p>
<pre class="codehilite"><code class="language-python"># LiDAR点云投影到图像获取深度真值
def project_lidar_to_image(points, T_cam_lidar, K):
    # 坐标变换
    points_cam = T_cam_lidar @ points.T

    # 投影
    uv = K @ points_cam[:3]
    uv = uv[:2] / uv[2]

    # 深度图
    depth_map = scatter_max(points_cam[2], uv)
    return depth_map
</code></pre>

<p>问题与解决：</p>
<ul>
<li>稀疏性: 插值或深度补全网络</li>
<li>时间同步: 硬件触发 + 软件补偿</li>
<li>标定精度: 在线标定算法</li>
</ul>
<h4 id="_15">困难样本挖掘</h4>
<pre class="codehilite"><code>困难场景识别:

- 强光/逆光: HDR增强
- 雨雾天气: 去雾网络
- 夜晚场景: 多曝光融合
- 动态物体: 运动分割

自动化挖掘pipeline:

1. 模型推理
2. 不确定性估计
3. 人工复核
4. 重新训练
</code></pre>

<h3 id="642">6.4.2 模型部署优化</h3>
<h4 id="tensorrt">TensorRT优化</h4>
<pre class="codehilite"><code class="language-python"># FP16推理优化
config.set_flag(trt.BuilderFlag.FP16)

# 动态批处理
profile.set_shape(&quot;input&quot;, 
    min=(1,3,384,1280),
    opt=(4,3,384,1280), 
    max=(8,3,384,1280))

# Layer融合

- Conv+BN+ReLU → CBR
- 多个1×1卷积 → Group Conv
</code></pre>

<p>性能提升:</p>
<ul>
<li>FP32→FP16: 1.8× 加速</li>
<li>算子融合: 1.3× 加速</li>
<li>动态批处理: 1.2× 吞吐提升</li>
</ul>
<h4 id="_16">多任务调度</h4>
<pre class="codehilite"><code>任务优先级调度:
┌─────────────────────────┐
│  高优先级 (10ms)         │
│  - 前向碰撞检测          │
│  - 紧急制动              │
├─────────────────────────┤
│  中优先级 (33ms)         │
│  - 3D检测                │
│  - 车道线检测            │
├─────────────────────────┤
│  低优先级 (100ms)        │
│  - 语义分割              │
│  - 停车位检测            │
└─────────────────────────┘
</code></pre>

<h3 id="643">6.4.3 系统集成挑战</h3>
<h4 id="_17">传感器时空同步</h4>
<pre class="codehilite"><code>时间戳对齐:
Camera: 30Hz ──┐
              ├→ 统一时间轴 (100Hz)
LiDAR: 10Hz ───┘

同步策略:

- PTP时钟同步 (&lt;1ms误差)
- 触发器硬同步
- 软件插值补偿
</code></pre>

<h4 id="_18">坐标系统一</h4>
<pre class="codehilite"><code>坐标系转换链:
像素坐标系 → 相机坐标系 → 车体坐标系 → 世界坐标系
   (u,v)      (x,y,z)_cam   (x,y,z)_ego   (lat,lon,alt)
     ↓            ↓              ↓              ↓
   内参K      外参T_ego_cam   定位系统      高精地图
</code></pre>

<h2 id="65">6.5 性能评估与对标</h2>
<h3 id="651">6.5.1 评价指标体系</h3>
<h4 id="2d">2D检测指标</h4>
<ul>
<li>mAP@IoU=0.5: 主流指标</li>
<li>mAP@IoU=0.5:0.95: 更严格</li>
<li>FPS: 实时性要求 &gt;20</li>
</ul>
<h4 id="3d_1">3D检测指标</h4>
<ul>
<li>3D AP: 3D IoU阈值 (0.7 car, 0.5 ped)</li>
<li>BEV AP: 鸟瞰图IoU</li>
<li>AOS: 考虑朝向的AP</li>
</ul>
<h4 id="_19">深度估计指标</h4>
<ul>
<li>RMSE: 均方根误差</li>
<li>REL: 相对误差</li>
<li>δ&lt;1.25: 准确度阈值</li>
</ul>
<h3 id="652-2020">6.5.2 主流方案对比 (2020年技术水平)</h3>
<p>| 公司/方案 | 2D mAP | 3D AP | 深度RMSE | FPS | 硬件平台 |</p>
<table>
<thead>
<tr>
<th>公司/方案</th>
<th>2D mAP</th>
<th>3D AP</th>
<th>深度RMSE</th>
<th>FPS</th>
<th>硬件平台</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tesla FSD</td>
<td>92.3</td>
<td>-</td>
<td>~3.5m</td>
<td>36</td>
<td>HW3.0</td>
</tr>
<tr>
<td>MobileEye</td>
<td>89.7</td>
<td>35.2</td>
<td>4.2m</td>
<td>100</td>
<td>EyeQ5</td>
</tr>
<tr>
<td>小鹏</td>
<td>88.5</td>
<td>31.8</td>
<td>4.8m</td>
<td>30</td>
<td>Xavier</td>
</tr>
<tr>
<td>地平线</td>
<td>87.2</td>
<td>28.5</td>
<td>5.1m</td>
<td>25</td>
<td>J3</td>
</tr>
<tr>
<td>Apollo</td>
<td>90.1</td>
<td>41.3*</td>
<td>3.8m*</td>
<td>20</td>
<td>GPU+LiDAR</td>
</tr>
</tbody>
</table>
<p>*注: Apollo使用激光雷达融合</p>
<h2 id="66">6.6 本章小结</h2>
<p>2016-2020年是自动驾驶感知技术的关键转型期。深度学习不仅取代了传统CV方法，更重要的是开启了端到端学习的新范式。从2D到3D感知的跨越、多任务学习的兴起、伪激光雷达的创新，这些技术突破为后续的BEV感知和端到端驾驶奠定了基础。</p>
<p>关键启示：</p>
<ol>
<li><strong>表示学习的重要性</strong>: 伪激光雷达证明了表示形式比传感器本身更关键</li>
<li><strong>多任务协同</strong>: 共享特征不仅节省算力，还能提升性能</li>
<li><strong>数据驱动</strong>: 大规模数据和自监督学习降低了标注成本</li>
<li><strong>系统思维</strong>: 感知不是孤立模块，需要与下游任务协同设计</li>
</ol>
<p>下一章我们将深入探讨BEV感知革命，看看这些基础技术如何演化成统一的3D感知框架。</p>
            </article>
            
            <nav class="page-nav"><a href="chapter5.html" class="nav-link prev">← 第5章：端到端浪潮 (2023-2024)</a><a href="chapter7.html" class="nav-link next">第7章：BEV感知革命与占据网络 →</a></nav>
        </main>
    </div>
</body>
</html>