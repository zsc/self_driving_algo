# 第8章：规划算法 - 从规则到学习

## 引言

规划算法是自动驾驶系统的"大脑中枢"，负责在复杂动态环境中生成安全、舒适、高效的驾驶轨迹。从2016年到2024年，规划算法经历了从纯规则驱动到数据驱动、从确定性算法到概率性推理、从单体智能到多智能体博弈的深刻变革。本章将深入剖析这一演进历程中的关键技术突破、工程实践和未来趋势。

## 1. 传统规划方法的基石与局限

### 1.1 基于搜索的规划算法

#### A*算法在自动驾驶中的应用

A*算法作为最经典的图搜索算法，在早期自动驾驶系统中扮演了重要角色。其核心思想是通过启发式函数引导搜索方向，在保证最优性的同时提高搜索效率。

```
A*算法在自动驾驶中的典型应用架构：

┌─────────────────────────────────────────────────┐
│                  环境表征层                      │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │ 栅格地图 │  │ 语义地图 │  │ 成本地图 │    │
│  └──────────┘  └──────────┘  └──────────┘    │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│                  A*搜索引擎                     │
│  ┌──────────────────────────────────────┐      │
│  │  f(n) = g(n) + h(n)                  │      │
│  │  g(n): 起点到n的实际代价              │      │
│  │  h(n): n到终点的启发式估计            │      │
│  └──────────────────────────────────────┘      │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│                 轨迹后处理                       │
│  • 路径平滑 (Cubic Spline, Bezier)             │
│  • 速度规划 (S-T图优化)                        │
│  • 碰撞检测与避让                              │
└─────────────────────────────────────────────────┘
```

**工程实践要点：**

1. **分辨率权衡**：栅格地图分辨率直接影响搜索空间大小
   - 高分辨率(0.1m)：精度高但计算量大，适合低速场景
   - 低分辨率(0.5-1m)：计算快但可能错过可行解

2. **启发式函数设计**：
   - 欧氏距离：简单快速但过于乐观
   - 曼哈顿距离：适合结构化道路
   - Dubins/Reeds-Shepp距离：考虑车辆运动学约束

3. **实时性优化**：
   - Anytime A*：可随时返回次优解
   - D* Lite：动态环境下的增量式重规划
   - Hybrid A*：连续空间与离散搜索的结合

**Apollo早期版本(2.0-3.0)的A*实现特点：**
- 采用3层搜索策略：粗搜索→精搜索→轨迹优化
- 引入动态障碍物预测，将时间维度加入搜索空间
- 使用启发式剪枝减少搜索空间

#### RRT系列算法的演进

快速扩展随机树(RRT)通过随机采样探索配置空间，特别适合高维空间和复杂约束下的规划问题。

```
RRT算法家族演进图谱：

         RRT (1998)
            ↓
      ┌─────┴─────┐
      ↓           ↓
   RRT*        RRT-Connect
   (2010)        (2000)
      ↓           ↓
   RRT*-Smart   Bi-RRT
   (2013)       (2001)
      ↓
   Informed RRT*
   (2014)
      ↓
   RRT# (2015)
   实时重规划
```

**关键技术改进：**

1. **RRT* (2010)**：渐进最优性保证
   - 重连线(Rewiring)机制
   - 收敛到最优解的理论保证
   - 计算复杂度：O(n log n)

2. **Informed RRT* (2014)**：利用已找到解引导采样
   - 椭圆采样区域
   - 快速收敛到更优解
   - Tesla AP2.x早期版本采用类似思想

3. **Kinodynamic RRT***：考虑动力学约束
   - 状态空间包含速度、加速度
   - 前向积分验证可行性
   - 适合高速场景规划

**工业界实践案例 - Waymo的混合RRT方案(2017-2019)：**
- 分层规划：道路级RRT + 轨迹级优化
- 并行化：多线程探索不同区域
- 启发式偏置：70%目标导向，30%随机探索

### 1.2 基于优化的轨迹规划

#### Lattice Planner的广泛应用

Lattice规划器通过在状态空间中预定义一组可行的运动原语(Motion Primitives)，将连续规划问题转化为图搜索问题。

```
Lattice Planner工作原理：

状态空间定义：
┌────────────────────────────────────┐
│  State = [x, y, θ, v, a, κ]       │
│  x,y: 位置  θ: 航向               │
│  v: 速度    a: 加速度  κ: 曲率    │
└────────────────────────────────────┘
            ↓
运动原语生成：
┌────────────────────────────────────┐
│     前进类     转向类     变道类    │
│   ──────→   ╱──────╲   ╱────────   │
│            ╱        ╲  ────────╲   │
└────────────────────────────────────┘
            ↓
轨迹评估与选择：
Cost = w₁·Jsafety + w₂·Jcomfort + w₃·Jefficiency
```

**Mercedes-Benz (2013-2018)的Lattice实践：**
- 横向规划：5个采样点(±3m, ±1.5m, 0m)
- 纵向规划：速度-时间多项式采样
- 评价函数：15个子目标加权组合
- 实时性：50Hz更新频率

**百度Apollo EM Planner架构(2018-2020)：**
```
┌─────────────────────────────────────┐
│         EM Planner Pipeline         │
├─────────────────────────────────────┤
│  1. 参考线提供器 (Reference Line)   │
│     - 基于高精地图                  │
│     - QP样条平滑                    │
├─────────────────────────────────────┤
│  2. 交通决策 (Traffic Decision)     │
│     - 车道选择                      │
│     - 障碍物标签                    │
├─────────────────────────────────────┤
│  3. 路径规划 (DP + QP)              │
│     - DP: 离散采样                  │
│     - QP: 二次规划优化              │
├─────────────────────────────────────┤
│  4. 速度规划 (ST Graph)             │
│     - 动态规划生成种子              │
│     - 二次规划平滑                  │
└─────────────────────────────────────┘
```

### 1.3 数值优化方法

#### 二次规划(QP)在轨迹平滑中的应用

```
典型QP问题形式：
min  J = x^T H x + f^T x
s.t. A_eq · x = b_eq
     A_ineq · x ≤ b_ineq
     lb ≤ x ≤ ub

在轨迹优化中：
- x: 轨迹控制点
- H: 平滑性权重矩阵
- 约束: 动力学限制、避障、道路边界
```

**小鹏NGP(2020)的QP优化器：**
- 使用OSQP求解器
- 分段B样条表示
- 实时求解(<10ms)

#### 非线性优化(NLP)

**IPOPT在Apollo中的应用：**
- 处理非凸避障约束
- 考虑车辆动力学模型
- 计算时间：20-50ms

### 1.4 传统方法的根本局限

尽管传统规划方法在确定性环境下表现良好，但面临以下根本性挑战：

1. **组合爆炸**：多智能体交互场景下状态空间指数增长
2. **手工规则局限**：难以穷举所有驾驶场景
3. **社会规范建模**：无法学习隐式驾驶习惯
4. **计算复杂度**：实时性与最优性的固有矛盾

## 2. 学习型规划的兴起与突破

### 2.1 模仿学习(Imitation Learning)演进

#### 行为克隆(Behavior Cloning)的早期探索

**NVIDIA DAVE-2 (2016)里程碑：**
```
架构极简但影响深远：
┌─────────────────────────────────┐
│   输入: 前视摄像头图像            │
│         ↓                       │
│   CNN特征提取                    │
│   (5 Conv + 3 FC)               │
│         ↓                       │
│   输出: 方向盘转角               │
└─────────────────────────────────┘

关键创新：
- 端到端学习，无需手工特征
- 仅用72小时驾驶数据训练
- 泛化到未见过的道路
```

**Waymo ChauffeurNet (2018)：**
```
输入表征创新 - Perception Feature Map：
┌────────────────────────────────────┐
│  通道1: 道路地图                    │
│  通道2: 交通灯状态                  │
│  通道3: 限速信息                    │
│  通道4: 当前路径                    │
│  通道5: 动态障碍物                  │
│  通道6: 历史轨迹                    │
└────────────────────────────────────┘
         ↓
    ConvLSTM处理时序
         ↓
    输出未来轨迹点
```

关键技术突破：
- **合成扰动增强**：人工生成困难场景
- **因果混淆缓解**：通过dropout随机遮挡
- **闭环仿真评估**：不只是开环指标

#### 逆强化学习(Inverse RL)

**Berkeley DeepDrive (2017-2019)的Maximum Entropy IRL：**
```
专家轨迹 → 推断奖励函数 → 生成类人轨迹

P(τ) ∝ exp(R(τ))
其中R(τ)通过神经网络学习

优势：
- 学习隐式偏好
- 处理多模态行为
- 更好的分布外泛化
```

### 2.2 强化学习在规划中的应用

#### 深度强化学习的突破

**特斯拉FSD Beta (2020-2022)的RL组件：**
```
奖励函数设计(推测)：
R = -w₁·碰撞风险 
    -w₂·车道偏离
    -w₃·急刹急转
    +w₄·行驶进度
    +w₅·舒适度分数

训练策略：
1. 离线RL预训练(大规模车队数据)
2. 仿真环境微调
3. 影子模式验证
4. 逐步部署
```

**Waymo规划器中的MCTS+RL (2021)：**
```
蒙特卡洛树搜索增强：
         根节点
         /   \
      动作1  动作2
      /  \    /  \
    ...  ... ...  ...
    
每个节点：
- 状态价值：V(s)由神经网络估计
- 访问次数：N(s,a)
- 选择策略：UCB1公式
```

#### 离线强化学习的实践

**问题**：在线RL在自动驾驶中风险太高

**解决方案 - Conservative Q-Learning (CQL)：**
```
核心思想：对未见过的动作保守估计

Q_CQL = Q_标准 - α·log∑exp(Q(s,a))
                    a

确保学到的策略不会偏离数据分布太远
```

**Momenta (2021)的离线RL实践：**
- 10亿帧历史驾驶数据
- CQL + IQL混合训练
- 泛化到新城市成功率85%+

### 2.3 混合方法的工程实践

#### 规则引导的学习规划

**小鹏XNGP (2023)架构：**
```
┌─────────────────────────────────┐
│      场景识别器(Transformer)     │
│  识别当前驾驶场景类型            │
└─────────────────────────────────┘
            ↓
     ┌──────┴──────┐
     ↓             ↓
┌─────────┐  ┌─────────┐
│规则规划器│  │神经规划器│
│(紧急情况)│  │(常规驾驶)│
└─────────┘  └─────────┘
     ↓             ↓
     └──────┬──────┘
            ↓
    ┌───────────────┐
    │  安全检查器    │
    │  硬约束验证    │
    └───────────────┘
```

**华为ADS 2.0 (2023)的分层决策：**
1. **高层策略网络**：选择驾驶意图
2. **中层战术规划**：生成参考轨迹
3. **底层轨迹优化**：考虑动力学约束

#### 数据驱动与模型驱动的结合

**Apollo 7.0 (2022) PNC-Net：**
```
输入编码：
┌──────────────────────────┐
│  栅格化BEV特征图          │
│  + 矢量化道路拓扑        │
│  + 智能体历史轨迹        │
└──────────────────────────┘
           ↓
    Transformer编码器
           ↓
┌──────────────────────────┐
│  输出1: 轨迹提议(学习)    │
│  输出2: 成本地图(学习)    │
└──────────────────────────┘
           ↓
    传统优化器精修
```

### 2.4 数据效率与泛化性挑战

#### 数据增强技术

**几何增强：**
- 轨迹扰动：添加高斯噪声
- 时间扭曲：改变速度剖面
- 空间变换：镜像、旋转

**对抗生成：**
```python
# 伪代码示例
adversarial_scene = baseline_scene
for step in range(max_steps):
    gradient = compute_difficulty_gradient(scene)
    adversarial_scene += ε * gradient
    if safety_violated(adversarial_scene):
        break
```

**场景重组：**
- 从不同场景提取智能体
- 组合成新的交互场景
- 保持物理合理性

#### 元学习与快速适应

**MAML在规划中的应用(Stanford, 2020)：**
```
元训练：
for task in tasks:
    θ' = θ - α∇L_task(θ)  # 内循环
    meta_loss += L_task(θ')
θ = θ - β∇meta_loss      # 外循环

效果：
- 新城市3小时数据即可适应
- 相比从头训练提升10倍效率
```

## 3. 轨迹预测与交互式规划

### 3.1 多智能体行为预测

#### 从独立预测到联合预测

**早期方法(2016-2018) - 独立预测：**
```
每个智能体独立建模：
Agent_i → LSTM/GRU → Future_trajectory_i

问题：
- 忽略交互
- 预测冲突
- 累积误差
```

**Social LSTM/GAN (2018-2020)：**
```
社交池化机制：
┌─────────────────────────────┐
│  Agent_1 ─→ LSTM_1 ←─┐      │
│                      ↓      │
│  Agent_2 ─→ LSTM_2 ← Pool   │
│                      ↑      │
│  Agent_3 ─→ LSTM_3 ←─┘      │
└─────────────────────────────┘

创新：
- 隐状态共享
- 空间关系编码
- 生成多模态预测
```

**VectorNet (2020, Waymo)：**
```
矢量化表示：
道路元素 → 折线段 → 子图编码
智能体轨迹 → 折线段 → 子图编码
         ↓
    全局交互图
         ↓
    GNN消息传递
         ↓
   多模态轨迹输出
```

关键优势：
- 处理不规则道路拓扑
- 长程依赖建模
- 计算效率高

#### Scene Transformer架构演进

**Waymo (2021) Scene Transformer：**
```
输入token化：
┌──────────────────────────────────┐
│  时间步1: [Agent₁, Agent₂, ...]  │
│  时间步2: [Agent₁, Agent₂, ...]  │
│  ...                             │
└──────────────────────────────────┘
            ↓
    Factorized Attention:
    1. 时间自注意力(per agent)
    2. 空间自注意力(per timestep)
    3. 全局自注意力
            ↓
     联合轨迹分布
```

性能提升：
- minADE: 降低30%
- 碰撞率：降低50%
- 推理速度：40ms@8agents

**Tesla FSD V11 (2022)的预测架构(推测)：**
```
多尺度时空Transformer：
┌─────────────────────────────┐
│   短程预测头(0-2s)          │
│   - 高频更新               │
│   - 细粒度轨迹             │
├─────────────────────────────┤
│   中程预测头(2-5s)          │
│   - 意图识别               │
│   - 交互建模               │
├─────────────────────────────┤
│   长程预测头(5-10s)         │
│   - 目标推断               │
│   - 粗粒度路径             │
└─────────────────────────────┘
```

### 3.2 博弈论方法在规划中的应用

#### 多智能体博弈建模

**Level-K推理模型：**
```
Level-0: 随机/反应式行为
Level-1: 假设他人是Level-0，最优响应
Level-2: 假设他人是Level-1，最优响应
...

实际应用(Uber ATG, 2019)：
- 大部分驾驶员: Level-1到Level-2
- 激进驾驶员: Level-0
- 谨慎驾驶员: Level-3
```

**逆向归纳(Backward Induction)：**
```
游戏树示例 - 并线场景：
        自车并线?
       /        \
    并线        不并线
    /  \          |
  让行 不让    继续直行
   |    |         |
  成功 碰撞     保持现状

通过逆向推理计算纳什均衡
```

#### 社会价值取向(SVO)建模

**MIT (2020)的SVO-based Planning：**
```
SVO角度分布：
利他 ←──────────→ 利己
-45°     0°      45°

驾驶员分类：
- 合作型(60%): SVO ∈ [-22.5°, 22.5°]
- 利己型(30%): SVO ∈ [22.5°, 45°]
- 竞争型(10%): SVO > 45°

规划策略：
根据估计的SVO调整自车行为激进度
```

### 3.3 交互感知规划

#### 条件预测与规划耦合

**Joint Prediction and Planning (JPP)：**
```
传统解耦方式：
预测 → 规划 (单向)

JPP耦合方式：
┌─────────────────────┐
│   预测  ←→  规划     │
│   相互影响，迭代优化 │
└─────────────────────┘

实现方式：
for iteration in range(K):
    他车轨迹 = 预测(自车规划)
    自车规划 = 优化(他车轨迹)
```

**INTERACTION Dataset挑战赛优胜方案(2022)：**
- 显式建模意图：左转/直行/右转
- 条件VAE生成多样化轨迹
- 社交力模型约束

#### 可解释的交互规划

**Waymo的可解释框架(2023)：**
```
交互图构建：
节点：智能体
边：潜在交互

边权重类型：
- 空间邻近度
- 速度相关性
- 历史交互模式
- 意图相似度

可视化输出：
┌────────────────────────┐
│  "Agent2正在让行"      │
│  "Agent3可能抢道"      │
│  "建议减速避让"        │
└────────────────────────┘
```

### 3.4 规划中的不确定性处理

#### 概率规划方法

**置信度传播：**
```
不确定性来源：
1. 感知不确定性 ±0.2m
2. 预测不确定性 ±1.0m@3s
3. 控制不确定性 ±0.1m

传播方式：
蒙特卡洛采样 or 高斯传播
```

**风险感知规划指标：**
```
CVaR (Conditional Value at Risk):
不考虑最坏5%情况下的平均风险

应用：
轨迹评分 = α·期望收益 - β·CVaR风险
```

## 4. 产业实践对比分析

### 4.1 主要厂商技术路线对比

| 厂商 | 规划架构 | 核心技术 | 数据规模 | 部署状态 |
|------|---------|----------|----------|----------|
| **Tesla FSD** | 端到端神经网络 | Transformer+占据网络 | 100亿英里 | 北美40万+用户 |
| **Waymo** | 混合架构 | 行为预测+优化 | 2000万英里路测 | 4城市Robotaxi |
| **小鹏XNGP** | 模块化+学习 | BEV+轻地图 | 10亿公里 | 全国推送 |
| **华为ADS** | 分层决策 | GOD网络+规则 | 未公开 | 问界/极狐量产 |
| **Momenta** | 双线并行 | 数据驱动闭环 | 数亿公里 | L2量产+L4测试 |
| **Apollo** | 开源模块化 | EM Planner | 1亿公里仿真 | Robotaxi试运营 |

### 4.2 关键技术演进时间线

```
2016 ├─ 纯规则规划主导
     │  A*, RRT, 多项式轨迹
     │
2017 ├─ 行为克隆初探
     │  NVIDIA DAVE-2影响力扩散
     │
2018 ├─ 模仿学习改进
     │  ChauffeurNet展示潜力
     │
2019 ├─ 预测规划分离成熟
     │  Social LSTM/GAN兴起
     │
2020 ├─ 图神经网络应用
     │  VectorNet矢量化表示
     │
2021 ├─ Transformer渗透
     │  Scene Transformer联合预测
     │
2022 ├─ 混合架构成主流
     │  学习+优化结合
     │
2023 ├─ 端到端规划爆发
     │  FSD V12纯神经网络
     │
2024 ├─ 世界模型探索
     └─ 生成式规划研究
```

### 4.3 技术挑战与解决方案矩阵

| 挑战 | 传统方法 | 学习方法 | 混合方案 |
|------|---------|----------|----------|
| **长尾场景** | ❌ 规则难穷举 | ⚠️ 数据稀疏 | ✅ 规则兜底+学习泛化 |
| **实时性** | ⚠️ 复杂度高 | ✅ 前向推理快 | ✅ 分层决策 |
| **可解释性** | ✅ 逻辑清晰 | ❌ 黑盒 | ⚠️ 部分可解释 |
| **安全保证** | ✅ 形式化验证 | ❌ 概率保证 | ⚠️ 安全边界约束 |
| **数据需求** | ✅ 无需数据 | ❌ 海量数据 | ⚠️ 中等规模 |

### 4.4 中国特色场景的规划挑战

#### 复杂交通流处理

**中国城市道路特点：**
```
挑战场景分布：
├─ 非机动车混行 (35%)
├─ 行人横穿 (25%)
├─ 外卖/快递穿插 (20%)
├─ 违章车辆 (15%)
└─ 施工/事故 (5%)

应对策略：
1. 多模态意图识别
2. 保守型安全边界
3. 社会规范学习
```

**小鹏XNGP城市场景处理：**
- 特殊目标识别：外卖车、三轮车
- 中国式加塞处理：渐进式让行
- 无保护左转：爬行策略

#### 高密度交互场景

**华为ADS 2.0的解决方案：**
```
时空联合规划：
┌─────────────────────────┐
│  空间维度：多车道选择    │
│  时间维度：加减速时机    │
│  交互维度：博弈预测     │
└─────────────────────────┘
        ↓
   4D成本地图优化
        ↓
   最优时空轨迹
```

### 4.5 算力与实时性权衡

#### 边缘计算约束

**典型算力预算分配：**
```
总算力: 100-200 TOPS
├─ 感知: 40-50%
├─ 预测: 20-25%
├─ 规划: 20-25%
└─ 控制: 5-10%

规划模块细分：
├─ 行为决策: 5 TOPS
├─ 轨迹生成: 10 TOPS
├─ 安全检查: 5 TOPS
└─ 备份方案: 5 TOPS
```

**优化策略：**
1. **模型量化**：FP32→INT8，性能损失<1%
2. **稀疏化**：剪枝70%参数，速度提升3x
3. **分级调度**：关键路径10Hz，次要路径2Hz
4. **区域裁剪**：只处理相关区域

### 4.6 数据闭环最佳实践

#### Tesla的数据飞轮

```
数据收集 → 自动标注 → 模型训练 → 影子模式
    ↑                              ↓
    └──────── 触发条件 ←───────────┘

触发条件示例：
- 人工接管
- 预测误差>阈值
- 新场景检测
- 不确定性高
```

#### 国内厂商的数据策略

**小鹏汽车：**
- 用户授权后收集
- 脱敏处理
- 仿真重建训练

**理想汽车：**
- 场景挖掘系统
- 自动化标注流水线
- 增量学习部署

## 5. 未来技术趋势展望

### 5.1 大语言模型驱动的规划

#### GPT-4V/Claude在规划中的探索

```
LLM辅助规划架构：
┌──────────────────────────┐
│   场景理解(VLM)          │
│   "前方施工，建议变道"    │
└──────────────────────────┘
            ↓
┌──────────────────────────┐
│   常识推理(LLM)          │
│   交通规则+社会规范      │
└──────────────────────────┘
            ↓
┌──────────────────────────┐
│   规划决策生成           │
│   自然语言→结构化指令    │
└──────────────────────────┘
```

**DriveGPT (毫末智行, 2023)：**
- 120亿参数视觉语言模型
- 场景理解+决策解释
- 4000万公里数据训练

### 5.2 世界模型与想象力规划

#### 预测未来的生成模型

**GAIA-1 (Wayve, 2023)：**
```
输入: 历史视频+动作
     ↓
世界模型(扩散模型)
     ↓
输出: 未来场景视频

应用：
1. 反事实推理
2. 闭环评估
3. 场景生成
```

**UniSim (Waabi, 2023)：**
- 神经场景表示
- 可控场景生成
- 物理真实渲染

### 5.3 端到端规划的极限探索

#### 纯神经网络规划的边界

**优势：**
- 无需手工设计
- 持续改进
- 处理复杂交互

**挑战：**
- 安全认证困难
- 极端场景泛化
- 法规合规性

#### 混合架构的长期主导

```
预期演进路径：
2024: 70%规则 + 30%学习
2025: 50%规则 + 50%学习  
2026: 30%规则 + 70%学习
2027+: 动态自适应比例
```

### 5.4 规划系统的形式化验证

#### 安全性证明方法

**形式化方法应用：**
```
模型检查：
规划器 → 抽象模型 → 性质验证
              ↓
        安全性证明/反例

符号执行：
探索所有可能路径
证明无碰撞/死锁
```

**ISO 26262/21448合规：**
- ASIL-D级别要求
- SOTIF安全分析
- 失效模式分析

## 6. 关键经验总结

### 6.1 工程实践要点

1. **分层架构设计**
   - 战略层：路线规划
   - 战术层：行为决策  
   - 操作层：轨迹生成

2. **冗余安全设计**
   - 主规划器+备份规划器
   - 规则兜底保证
   - 紧急制动系统

3. **实时性优化**
   - 异步并行架构
   - 增量式计算
   - 自适应降级策略

### 6.2 数据驱动最佳实践

1. **场景挖掘**
   - 自动发现corner case
   - 主动学习策略
   - 困难样本重采样

2. **仿真验证**
   - 蒙特卡洛测试
   - 对抗场景生成
   - 回归测试自动化

3. **持续优化**
   - A/B测试框架
   - 指标体系设计
   - 用户反馈闭环

### 6.3 技术选型建议

| 场景 | 推荐方案 | 原因 |
|------|---------|------|
| L2高速 | Lattice+优化 | 成熟可靠 |
| L2城市 | 混合学习 | 平衡复杂度 |
| L4限定区域 | 模块化+冗余 | 安全优先 |
| 泊车 | 端到端 | 场景受限 |

## 结语

规划算法作为自动驾驶的核心决策模块，正在经历从规则驱动到数据驱动的深刻变革。传统方法提供了坚实的理论基础和安全保证，而学习方法展现了处理复杂交互和长尾场景的巨大潜力。

未来的规划系统将是混合智能的集大成者：既有规则的确定性，又有学习的灵活性；既能处理日常驾驶，又能应对极端情况。随着算力提升、数据积累和算法创新，规划系统将越来越接近人类驾驶员的决策水平，最终实现真正的自主驾驶。

从2016年的规则主导到2024年的端到端浪潮，规划算法的演进见证了人工智能技术在实际工程中的落地过程。这不仅是技术的进步，更是对复杂现实世界建模能力的提升。下一个突破或许来自世界模型、或许来自更强大的基础模型，但可以确定的是，规划算法仍将是自动驾驶技术竞争的核心战场。