# 第3章：感知架构大爆发 (2019-2020)

## 章节概述

2019-2020年是自动驾驶感知技术的转折点。这两年间，行业从简单的2D检测快速演进到复杂的3D感知，多任务学习网络开始大规模应用，BEV（鸟瞰图）感知范式初露端倪。特斯拉在2019年Autonomy Day上展示的FSD芯片和神经网络架构震撼业界，而2020年COVID疫情意外推动了无人配送的落地。同时，中国自动驾驶产业在这一时期快速崛起，NOA（Navigate on Autopilot）功能开始在多家车企落地。

## 3.1 2019 Tesla Autonomy Day：FSD芯片与算法协同设计的里程碑

### 3.1.1 FSD芯片架构革新

2019年4月22日，特斯拉举办了震撼业界的Autonomy Day。最引人注目的是其自研的FSD（Full Self-Driving）芯片，这标志着特斯拉彻底摆脱了对Mobileye和NVIDIA的依赖。

```
FSD芯片关键参数：
┌─────────────────────────────────────────┐
│         Tesla FSD Computer (HW3.0)      │
├─────────────────────────────────────────┤
│ 制程工艺：     14nm FinFET (三星代工)    │
│ 算力：         144 TOPS (INT8)          │
│ NPU数量：      2个独立NPU               │
│ 单NPU算力：    72 TOPS                  │
│ CPU：          12核ARM Cortex-A72       │
│ GPU：          1 GHz Mali G71 MP12      │
│ 内存：         8GB LPDDR4               │
│ 功耗：         72W (整板)               │
│ 成本：         ~$190 (量产成本)         │
└─────────────────────────────────────────┘
```

**芯片设计哲学**：

特斯拉FSD芯片的设计完全围绕神经网络推理优化，与通用GPU方案形成鲜明对比：

1. **专用神经网络加速器（NNA）设计**：
   - 96×96 MAC阵列，专门优化卷积运算
   - 支持INT8/FP16混合精度计算
   - 片上SRAM达32MB，减少DRAM访问
   - 硬件级别的批归一化（BN）和激活函数支持

2. **冗余安全设计**：
   ```
   双芯片冗余架构：
   ┌──────────┐     ┌──────────┐
   │  SoC A   │<--->│  SoC B   │
   │  72 TOPS │     │  72 TOPS │
   └──────────┘     └──────────┘
        ↓               ↓
   独立推理结果    独立推理结果
        ↓               ↓
   ┌─────────────────────────┐
   │    结果比对与仲裁器      │
   └─────────────────────────┘
   ```
   - 两个SoC完全独立运行相同神经网络
   - 实时比对推理结果，检测硬件故障
   - 满足ASIL-D功能安全要求

3. **与竞品的性能对比**（2019年数据）：

| 平台 | 算力(TOPS) | 功耗(W) | 能效比(TOPS/W) | 成本($) |
|------|-----------|---------|----------------|---------|
| Tesla FSD | 144 | 72 | 2.0 | 190 |
| NVIDIA Xavier | 30 | 30 | 1.0 | 350 |
| MobileEye EyeQ4 | 2.5 | 3 | 0.83 | 100 |
| 地平线征程2 | 4 | 2 | 2.0 | 50 |

**关键技术创新**：

1. **定制指令集架构（ISA）**：
   - 为神经网络运算定制的SIMD指令
   - 支持稀疏张量运算，提升实际吞吐量
   - 零拷贝DMA传输机制

2. **内存层次优化**：
   ```
   内存层次结构：
   L1 Cache (每个CPU核心) : 32KB
           ↓
   L2 Cache (共享) : 2MB  
           ↓
   片上SRAM : 32MB (NNA专用)
           ↓
   LPDDR4 : 8GB (68GB/s带宽)
   ```

3. **编译器与软件栈**：
   - PyTorch → ONNX → Tesla编译器
   - 自动算子融合和图优化
   - 支持动态批处理和多流并发

### 3.1.2 神经网络架构创新

特斯拉展示的神经网络架构相比之前有了质的飞跃：

**HydraNet多头网络架构**
```
                 共享骨干网络 (RegNet)
                        ↓
    ┌──────────┬──────────┬──────────┬──────────┐
    │          │          │          │          │
  检测头     车道线头    可行驶区域   交通标志   深度估计
   Head      Head        Head       Head       Head
    │          │          │          │          │
  输出1      输出2       输出3      输出4      输出5
```

关键创新点：
- **共享特征提取器**：使用RegNet作为骨干网络，参数量50M，相比ResNet-50减少了计算量但提升了准确率
- **多任务学习**：单个网络同时处理检测、分割、深度估计等多个任务
- **时序融合**：引入时间维度，利用视频连续帧提升检测稳定性

**RegNet骨干网络的选择**：

特斯拉选择RegNet而非流行的ResNet/EfficientNet有深层考虑：

1. **网络设计空间探索**：
   ```
   RegNet设计原则：
   宽度: w = 48 * (2.0)^i  (通道数递增)
   深度: d = [2, 5, 12, 4] (每阶段块数)
   组宽: g = 24           (分组卷积)
   
   优势：
   • 参数效率提升40%
   • 推理速度提升30%
   • 硬件友好的规整结构
   ```

2. **多尺度特征提取优化**：
   ```
   特征金字塔输出：
   Stage 1: 320×240×64   (1/4分辨率)
   Stage 2: 160×120×128  (1/8分辨率)  
   Stage 3: 80×60×256    (1/16分辨率)
   Stage 4: 40×30×512    (1/32分辨率)
   Stage 5: 20×15×1024   (1/64分辨率)
   ```

3. **任务头设计细节**：

| 任务头 | 输入特征 | 网络结构 | 输出规格 | 推理耗时 |
|--------|---------|----------|----------|----------|
| 物体检测 | Stage 3-5 | FPN+3×3 Conv | 100×75×(C+5) | 2.1ms |
| 车道线 | Stage 2-4 | U-Net decoder | 160×120×3 | 1.8ms |
| 可行驶区域 | Stage 2-3 | FCN | 80×60×2 | 1.2ms |
| 交通标志 | Stage 4-5 | RoI Align | 50类分类 | 0.9ms |
| 深度估计 | Stage 2-5 | Encoder-Decoder | 160×120×1 | 2.5ms |

**时序信息融合机制**：

特斯拉在2019年引入的时序融合是关键突破：

```
时序特征聚合：
t-3  t-2  t-1   t
 ↓    ↓    ↓    ↓
特征  特征  特征  特征
 ↓    ↓    ↓    ↓
[特征对齐模块]←─ 自车运动补偿
      ↓
[时序注意力机制]
      ↓
  融合特征
```

1. **特征对齐**：
   - 使用IMU和轮速计进行自车运动补偿
   - 光流估计进行像素级对齐
   - 可变形卷积适应物体形变

2. **时序建模**：
   - ConvLSTM捕获时序依赖
   - 时序注意力权重学习
   - 关键帧选择机制（减少冗余）

3. **性能提升**：
   - 检测稳定性提升35%
   - 遮挡物体召回率提升50%
   - 假阳性率降低40%

### 3.1.3 数据引擎与Shadow Mode

特斯拉首次详细介绍了其"影子模式"（Shadow Mode）数据收集系统：

1. **被动数据收集**：车辆在后台运行神经网络，但不控制车辆
2. **分歧检测**：当神经网络预测与人类驾驶行为不一致时，触发数据上传
3. **自动标注**：利用车队数据自动生成训练标签

数据规模（2019年）：
- 车队规模：超过50万辆配备FSD硬件的车辆
- 日均里程：2000万英里
- 月度数据量：1PB级别的视频数据

**Shadow Mode工作原理详解**：

```
影子模式数据流：
┌────────────────────────────────┐
│        生产车辆                 │
├────────────────────────────────┤
│  主控制系统 │ 影子系统          │
│  (AP 2.5)  │ (FSD Beta)        │
│     ↓      │     ↓             │
│  实际控制   │  仅推理不控制      │
│     ↓      │     ↓             │
│  CAN总线 ←─┴─→ 行为比对器       │
│                   ↓             │
│            触发条件判断          │
│                   ↓             │
│            数据上传队列          │
└────────────────────────────────┘
```

**触发数据收集的典型场景**：

1. **预测分歧场景**：
   - 人类驾驶员刹车，但系统未预测需要刹车
   - 系统建议变道，但驾驶员未执行
   - 轨迹预测与实际行驶路径偏差>2米

2. **边缘案例自动识别**：
   ```
   触发条件配置示例：
   {
     "hard_brake": {
       "deceleration": "> 5 m/s²",
       "confidence": "< 0.8",
       "upload_window": "[-5s, +10s]"
     },
     "object_disappear": {
       "tracking_loss": "true",
       "distance": "< 50m",
       "upload_window": "[-3s, +3s]"
     },
     "unusual_behavior": {
       "trajectory_deviation": "> 2m",
       "steering_rate": "> 100°/s"
     }
   }
   ```

3. **主动查询采集**：
   - OTA推送特定场景采集任务
   - 如："收集雨天高速变道数据"
   - 车队分布式执行，48小时内回传

**自动标注系统架构**：

```
多视角自动标注流程：
车辆A观测 ──┐
车辆B观测 ──┼──→ 时空对齐 → 3D重建 → 自动标注
车辆C观测 ──┘         ↓
                 HD Map校准
                      ↓
                 标注质量评分
                      ↓
              人工审核(< 5%)
```

关键技术：
1. **时空配准**：GPS时间戳 + 视觉SLAM
2. **多视角融合**：同一场景不同车辆的观测融合
3. **4D标注**：物体在时间维度的连续标注
4. **标注传播**：利用跟踪将标注传播到整个序列

**数据引擎迭代循环**：

```
版本迭代周期（2周）：
Day 1-3:   数据收集与清洗
Day 4-5:   自动标注生成
Day 6-7:   模型训练
Day 8-9:   仿真验证
Day 10-11: 实车测试
Day 12-13: A/B测试
Day 14:    OTA推送
```

特斯拉通过这套系统实现了：
- 标注成本降低90%
- 数据多样性提升10倍
- 长尾场景覆盖率从20%提升到80%

## 3.2 多任务学习网络兴起：从单一功能到统一架构

### 3.2.1 传统方案的局限性

2019年之前的自动驾驶感知系统通常采用独立模块设计：

```
传统模块化设计：
┌──────────┐  ┌──────────┐  ┌──────────┐
│ 车辆检测  │  │ 车道检测  │  │ 标志识别  │
│  模型1   │  │  模型2   │  │  模型3   │
└──────────┘  └──────────┘  └──────────┘
     ↓             ↓             ↓
  YOLO v3      LaneNet      Sign-CNN
  (30 FPS)     (20 FPS)     (15 FPS)

问题：
• 计算冗余：每个模型独立提取特征
• 内存占用大：多个模型同时加载
• 难以协同：模块间信息无法共享
```

### 3.2.2 多任务学习架构演进

**第一代：硬参数共享（Hard Parameter Sharing）**

最早期的多任务网络采用简单的硬参数共享：

```
输入图像
    ↓
共享编码器 (ResNet-50)
    ↓
┌────┴────┬────┴────┐
检测分支  分割分支  深度分支
```

代表工作：
- **MultiNet (2016)**：最早的端到端多任务驾驶网络
- **DLT-Net (2018)**：可行驶区域+车道线联合检测
- **YOLOP (2019)**：检测+可行驶区域+车道线三任务

**第二代：注意力机制增强**

2019年开始，注意力机制被引入多任务学习：

```
TaskPrompter架构 (2019)：
         输入特征
            ↓
    ┌───────────────┐
    │  任务注意力模块 │
    └───────────────┘
      ↙    ↓    ↘
  任务1  任务2  任务3
  权重   权重   权重
```

关键技术：
- **任务相关性建模**：学习不同任务间的依赖关系
- **动态权重调整**：根据场景自适应调整任务权重
- **梯度平衡**：解决多任务训练中的梯度冲突问题

### 3.2.3 产业应用案例

**Mobileye EyeQ5架构**

虽然特斯拉抛弃了Mobileye，但后者在2019年发布的EyeQ5展示了另一种多任务设计思路：

```
EyeQ5多任务处理单元：
┌─────────────────────────────────┐
│       EyeQ5 (24 TOPS)           │
├─────────────────────────────────┤
│ • 4个多线程CPU集群              │
│ • 18个视觉处理器(CVP)           │
│ • 深度学习加速器(DLA)           │
│ • 2个可编程宏阵列(PMA)          │
└─────────────────────────────────┘
         ↓
  并行处理15+个视觉任务
```

EyeQ5的多任务调度策略：
1. **硬件级任务分配**：
   - CVP处理传统CV任务（光流、特征点）
   - DLA处理深度学习任务
   - CPU处理高层融合逻辑

2. **任务优先级管理**：
   ```
   优先级队列：
   P0: 紧急制动相关 (AEB)     - 10ms响应
   P1: 车道保持 (LKA)         - 20ms响应
   P2: 目标检测与跟踪         - 33ms响应
   P3: 交通标志识别           - 100ms响应
   P4: 自由空间检测           - 100ms响应
   ```

3. **性能指标**（2019年测试）：
   - 同时处理15个摄像头输入
   - 运行40+个神经网络
   - 系统延迟<100ms
   - 功耗<10W

**地平线Matrix 2.0**

中国本土芯片厂商地平线在2019年推出的Matrix 2.0展示了高效的多任务处理：

- 4路摄像头输入
- 同时运行8个神经网络
- 单芯片实现360°感知

Matrix 2.0多任务架构创新：
```
征程2芯片架构：
┌────────────────────────────┐
│   BPU (Brain Process Unit) │
├────────────────────────────┤
│  双核架构 @ 1GHz           │
│  96个MAC单元               │
│  4 TOPS INT8算力           │
└────────────────────────────┘
           ↓
    高效任务调度器
   ┌────┬────┬────┬────┐
   │检测│分割│跟踪│测距│
   └────┴────┴────┴────┘
```

关键优化：
1. **模型压缩**：8bit量化 + 稀疏化，模型大小减少75%
2. **编译器优化**：自动算子融合，减少内存访问
3. **动态资源分配**：根据场景复杂度调整算力分配

**NVIDIA Xavier多任务方案**

作为对比，NVIDIA的通用GPU方案展示了不同思路：

```
Xavier Carmel GPU架构：
512个CUDA核心
     ↓
统一计算模型
     ↓
┌─────────────┐
│  TensorRT   │ - 推理优化
│  cuDNN      │ - 深度学习库
│  VisionWorks│ - 传统CV
└─────────────┘
```

优势：
- 灵活性高，易于开发
- 生态完善，工具链成熟
- 支持各种网络结构

劣势：
- 功耗较高（30W）
- 成本高（$350+）
- 需要主动散热

### 3.2.4 多任务学习的理论基础

**为什么多任务学习有效？**

1. **表征共享（Representation Sharing）**：
   ```
   信息层次：
   低层特征（边缘、纹理）── 所有任务共享
           ↓
   中层特征（部件、形状）── 部分任务共享
           ↓
   高层特征（语义、类别）── 任务特定
   ```

2. **归纳偏置（Inductive Bias）**：
   - 多任务约束网络学习更通用的特征
   - 减少过拟合风险
   - 提升泛化能力

3. **注意力聚焦（Attention Focusing）**：
   - 不同任务关注不同特征
   - 互补性增强鲁棒性
   - 如：检测关注边界，分割关注内部

**多任务学习的挑战与解决**：

1. **梯度冲突问题**：
   ```
   问题：不同任务梯度方向相反
   
   解决方案对比：
   ├─ 不确定性加权：σ²加权各任务损失
   ├─ 梯度归一化：GradNorm动态平衡
   ├─ 帕累托优化：寻找帕累托最优解
   └─ 任务交替：动态调整任务训练频率
   ```

2. **负迁移（Negative Transfer）**：
   - 现象：联合训练性能反而下降
   - 原因：任务相关性低或冲突
   - 解决：任务分组、选择性共享

3. **容量分配**：
   ```
   网络容量分配策略：
   共享层：70% 参数
   任务特定层：30% 参数
      ├─ 检测：12%
      ├─ 分割：10%
      └─ 其他：8%
   ```

## 3.3 伪激光雷达与深度估计：纯视觉3D感知的突破

### 3.3.1 伪激光雷达（Pseudo-LiDAR）概念

2019年康奈尔大学提出的Pseudo-LiDAR论文引发了业界对纯视觉3D感知的重新思考：

**核心思想**：
1. 从立体视觉或单目深度估计获得深度图
2. 将深度图转换为3D点云（伪激光雷达）
3. 使用成熟的点云检测算法进行3D目标检测

```
Pseudo-LiDAR处理流程：
┌──────────┐     ┌──────────┐     ┌──────────┐
│  左图像   │ --> │  深度估计 │ --> │ 3D点云   │
│  右图像   │     │  网络    │     │  生成    │
└──────────┘     └──────────┘     └──────────┘
                                         ↓
                                  ┌──────────┐
                                  │ PointNet │
                                  │ 3D检测   │
                                  └──────────┘
```

### 3.3.2 单目深度估计技术爆发

2019-2020年，单目深度估计取得重大突破：

**关键方法对比**：

| 方法 | 年份 | 特点 | KITTI深度误差 |
|------|------|------|--------------|
| Monodepth2 | 2019 | 自监督学习 | 0.115 |
| PackNet-SfM | 2019 | 3D卷积打包 | 0.107 |
| Depth Hints | 2019 | 立体监督 | 0.098 |
| FeatDepth | 2020 | 特征度量学习 | 0.088 |

**特斯拉的深度估计网络**

特斯拉在2019年展示的深度网络采用了独特的设计：

```
深度网络架构：
输入: 1280×960 RGB图像
        ↓
特征提取: RegNet骨干
        ↓
多尺度特征金字塔
   ↙    ↓    ↘
粗糙  中等  精细
深度  深度  深度
   ↘    ↓    ↙
    深度融合
        ↓
输出: 256×144 深度图
```

创新点：
- **自监督预训练**：利用时序一致性
- **语义引导**：物体类别信息辅助深度估计
- **边缘保持**：保持物体边界的深度不连续性

### 3.3.3 产业化挑战与解决方案

**挑战1：计算复杂度**
- 问题：深度估计网络计算量大
- 解决：模型量化、剪枝、知识蒸馏

**挑战2：尺度模糊性**
- 问题：单目深度缺乏绝对尺度
- 解决：利用已知物体尺寸、地面约束

**挑战3：远距离精度**
- 问题：远处物体深度误差大
- 解决：分段深度估计、注意力机制

## 3.4 2020年1月：德国批准Tesla Autopilot合法化的技术影响

### 3.4.1 监管认证要求

德国KBA（联邦机动车管理局）的认证要求推动了技术标准化：

```
认证技术要求：
┌──────────────────────────────────┐
│     功能安全 (ISO 26262)         │
├──────────────────────────────────┤
│ • ASIL-D等级系统设计             │
│ • 冗余传感器配置                 │
│ • 失效安全机制                   │
│ • 实时监控与降级策略             │
└──────────────────────────────────┘
```

### 3.4.2 技术适应性改进

为满足欧洲法规，特斯拉进行了多项技术改进：

1. **增强的驾驶员监控**
   - 方向盘扭矩检测频率提升
   - 注意力监测算法优化

2. **场景适应性**
   - 欧洲道路标志识别
   - 环岛处理逻辑
   - 窄路会车策略

3. **功能限制**
   - 自动变道需确认
   - 速度限制遵守
   - 施工区域检测增强

### 3.4.3 对全球市场的示范效应

德国的批准产生了连锁反应：
- 欧盟其他国家跟进
- 中国加速NOA功能审批
- 技术标准趋同化

## 3.5 2020 COVID推动无人配送发展：感知技术的新挑战

### 3.5.1 无人配送场景的独特需求

疫情期间，无人配送从概念快速走向应用，带来新的技术挑战：

```
配送场景 vs 乘用车场景：
┌───────────────┬──────────────────┐
│   无人配送     │     乘用车       │
├───────────────┼──────────────────┤
│ 速度: <30km/h │ 速度: 0-120km/h  │
│ 环境: 园区    │ 环境: 开放道路    │
│ 路线: 固定    │ 路线: 任意        │
│ 交互: 行人密集 │ 交互: 车辆为主    │
└───────────────┴──────────────────┘
```

### 3.5.2 低速场景的感知优化

**美团无人配送车感知方案**（2020年2月投入武汉）：

```
传感器配置：
     前视激光雷达(16线)
           ↓
    ┌──────────────┐
    │              │
环视 │   配送车     │ 环视
相机 │              │ 相机
    │              │
    └──────────────┘
           ↑
     超声波雷达阵列

特点：
• 成本控制在2万元以内
• 360°无死角感知
• 重点优化行人检测
```

**京东无人配送感知策略**：
1. 高精地图+视觉定位
2. 行人意图预测
3. 可通行空间实时更新

### 3.5.3 疫情期间的快速迭代

**数据积累加速**：
- 2020年2-6月，中国无人配送车累计运行超过100万公里
- 收集了大量"最后一公里"场景数据

**算法快速迭代**：
- 口罩佩戴下的行人检测
- 临时路障识别
- 消毒作业车辆避让

## 3.6 BEV感知初现端倪：从图像空间到鸟瞰图空间

### 3.6.1 BEV感知的动机

传统前视图感知的局限性逐渐显现：

```
前视图 vs BEV空间：

前视图问题：            BEV优势：
┌──────────┐         ┌──────────┐
│ 透视畸变  │         │ 度量准确  │
│ 遮挡严重  │   -->   │ 全局视角  │  
│ 多相机融合难│         │ 便于规划  │
└──────────┘         └──────────┘
```

### 3.6.2 早期BEV方法

**IPM（逆透视映射）时代**（2019年之前）：
- 假设地面平坦
- 简单的几何变换
- 仅适用于车道线、可行驶区域

**LSS（Lift, Splat, Shoot）突破**（2020年）：

```
LSS处理流程：
1. Lift: 图像特征 + 深度分布 → 3D特征
2. Splat: 3D特征 → BEV网格
3. Shoot: BEV特征 → 下游任务

     图像特征           
         ↓
    深度分布预测
         ↓
    3D特征云
         ↓
    体素化(Voxelize)
         ↓
    BEV特征图
```

关键创新：
- 显式的深度概率分布建模
- 可微的3D-2D投影
- 端到端训练

### 3.6.3 产业先驱探索

**特斯拉的Vector Space**（2019年开始内部研发）：

虽然特斯拉在2021年AI Day才公开BEV架构，但从2019年的专利和招聘信息可以看出其已在探索：

1. "Vector Space"概念：将感知结果投影到统一的3D空间
2. 多摄像头时空融合
3. 隐式的BEV表征学习

**百度Apollo的BEV尝试**：

```
Apollo 5.5 (2020年) BEV模块：
┌─────────────────────────┐
│   6个环视摄像头输入      │
└─────────────────────────┘
            ↓
    ┌───────────────┐
    │  特征提取网络  │
    └───────────────┘
            ↓
    ┌───────────────┐
    │  BEV转换模块   │
    └───────────────┘
            ↓
    ┌───────────────┐
    │  HD Map融合    │
    └───────────────┘
```

## 3.7 中国NOA功能开始落地：本土化创新与挑战

### 3.7.1 中国NOA先行者

2020年，中国多家车企开始推出NOA（Navigate on Autopilot）类功能：

**小鹏NGP（Navigation Guided Pilot）**（2020年10月）：
- 首个量产的高速自主导航辅助
- 覆盖自动超车、自动变道、自动上下匝道
- 基于高精地图的强依赖方案

**蔚来NOP（Navigate on Pilot）**（2020年10月）：
- Mobileye EyeQ4芯片方案
- 保守的安全策略
- 用户激活率超过50%

**理想NOA**（2020年12月）：
- 地平线征程3芯片
- 视觉为主的感知方案
- 成本控制在3000元以内

### 3.7.2 本土化技术创新

**中国特色场景处理**：

```
中国道路挑战：
┌────────────────────────────┐
│ 1. 加塞频繁                │
│ 2. 车道线不清晰            │
│ 3. 施工区域多              │
│ 4. 非标准道路设计          │
│ 5. 混合交通流              │
└────────────────────────────┘
```

技术应对：
1. **激进的变道策略**：相比特斯拉更激进的变道时机判断
2. **高精地图依赖**：弥补车道线检测的不足
3. **本地化训练数据**：大量中国道路场景数据

### 3.7.3 技术架构对比

| 厂商 | 芯片方案 | 感知方案 | 地图依赖 | 成本 |
|------|---------|----------|----------|------|
| 小鹏 | Xavier | 视觉+毫米波 | 强依赖 | ~5000元 |
| 蔚来 | EyeQ4 | 纯视觉 | 中等依赖 | ~8000元 |
| 理想 | 征程3 | 视觉为主 | 轻依赖 | ~3000元 |

### 3.7.4 用户接受度与迭代

**使用数据**（2020年底）：
- 小鹏NGP：日均使用里程超过20万公里
- 蔚来NOP：累计行驶超过1000万公里
- 月度OTA更新成为常态

**快速迭代特点**：
- 2周一次的算法优化
- 基于用户反馈的功能改进
- 影子模式数据收集

## 3.8 感知技术栈的全面升级

### 3.8.1 2019-2020年的关键技术突破总结

```
技术演进时间线：
2019 Q1 ├─ Pseudo-LiDAR论文发表
2019 Q2 ├─ Tesla Autonomy Day
2019 Q3 ├─ 多任务学习网络普及
2019 Q4 ├─ 深度估计突破0.1误差
2020 Q1 ├─ 德国批准Autopilot
2020 Q2 ├─ COVID推动无人配送
2020 Q3 ├─ LSS/BEV方法提出
2020 Q4 ├─ 中国NOA大规模落地
```

### 3.8.2 产业格局变化

**算力军备竞赛开始**：
- 2019年：30-100 TOPS成为主流
- 2020年：100-200 TOPS成为新标准
- 预告2021年：500+ TOPS平台出现

**数据成为核心竞争力**：
- 数据闭环的重要性凸显
- 影子模式成为标配
- 自动标注技术快速发展

### 3.8.3 未来趋势预判（2020年底的视角）

1. **BEV将成为主流**：多家公司开始投入BEV研发
2. **Transformer即将到来**：NLP的成功预示CV变革
3. **端到端探索加速**：模块化的瓶颈日益明显
4. **中美技术路线分化**：特斯拉纯视觉 vs 中国融合路线

## 本章小结

2019-2020年是自动驾驶感知技术从量变到质变的关键时期。特斯拉通过垂直整合展示了算法与芯片协同设计的威力，多任务学习网络的兴起大幅提升了系统效率，伪激光雷达和深度估计的突破证明了纯视觉3D感知的可行性。COVID疫情意外加速了无人配送的落地，而BEV感知范式的出现预示着下一代技术变革。中国市场在这一时期快速崛起，本土NOA功能的落地标志着中国自动驾驶产业进入快速发展期。

这两年奠定的技术基础，为2021-2022年的BEV和Transformer革命铺平了道路，也预示着端到端学习时代的到来。

---

*下一章：[第4章 BEV与Transformer变革 (2021-2022)](chapter4.md)*